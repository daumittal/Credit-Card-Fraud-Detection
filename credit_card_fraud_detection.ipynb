{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf2FBaRzu6gR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for credit card fraud data.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix.\n",
    "            labels (np.ndarray): Target labels (0 or 1).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class FraudDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for fraud detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32]):\n",
    "        super(FraudDetector, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "class FraudDetectionPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for credit card fraud detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str, output_dir: str = \"fraud_results\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (str): Path to the credit card dataset CSV.\n",
    "            output_dir (str): Directory to save outputs.\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load and validate the dataset.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading data from {self.data_path}\")\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "        expected_columns = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n",
    "                            'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "                            'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',\n",
    "                            'V28', 'Amount', 'Class']\n",
    "        if not all(col in self.data.columns for col in expected_columns):\n",
    "            raise ValueError(\"Dataset missing required columns\")\n",
    "        logger.info(f\"Dataset shape: {self.data.shape}\")\n",
    "\n",
    "    def visualize_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Visualize class distribution and feature correlations.\n",
    "        \"\"\"\n",
    "        logger.info(\"Generating visualizations\")\n",
    "        \n",
    "        # Class distribution\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(x='Class', data=self.data)\n",
    "        plt.title(\"Class Distribution (0: Non-Fraud, 1: Fraud)\")\n",
    "        plt.savefig(self.output_dir / \"class_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Correlation heatmap for selected features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        corr = self.data[['V1', 'V2', 'V3', 'V4', 'Amount', 'Class']].corr()\n",
    "        sns.heatmap(corr, annot=True, cmap=\"viridis\", fmt=\".2f\")\n",
    "        plt.title(\"Correlation Heatmap\")\n",
    "        plt.savefig(self.output_dir / \"correlation_heatmap.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def preprocess_data(self, test_size: float = 0.2, random_state: int = 42) -> None:\n",
    "        \"\"\"\n",
    "        Preprocess data: scale features, balance classes, and create data loaders.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Proportion of data for testing.\n",
    "            random_state (int): Seed for reproducibility.\n",
    "        \"\"\"\n",
    "        logger.info(\"Preprocessing data\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = self.data.drop('Class', axis=1)\n",
    "        y = self.data['Class']\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Balance training data with ADASYN\n",
    "        logger.info(\"Balancing training data with ADASYN\")\n",
    "        adasyn = ADASYN(random_state=random_state)\n",
    "        X_train_bal, y_train_bal = adasyn.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = FraudDataset(X_train_bal, y_train_bal)\n",
    "        test_dataset = FraudDataset(X_test, y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        logger.info(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    def initialize_model(self, input_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "        \"\"\"\n",
    "        self.model = FraudDetector(input_dim).to(self.device)\n",
    "        logger.info(\"Model initialized\")\n",
    "\n",
    "    def train_model(self, epochs: int = 50, lr: float = 0.001) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the model and track metrics.\n",
    "\n",
    "        Args:\n",
    "            epochs (int): Number of training epochs.\n",
    "            lr (float): Learning rate.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[float]]: Training and validation metrics history.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting training\")\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        history = {\"train_loss\": [], \"train_recall\": [], \"val_loss\": [], \"val_recall\": []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss, train_preds, train_labels = 0.0, [], []\n",
    "            \n",
    "            for features, labels in self.train_loader:\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_preds.extend((outputs >= 0.5).float().cpu().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_loss /= len(self.train_loader)\n",
    "            train_recall = recall_score(train_labels, train_preds)\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss, val_preds, val_labels = 0.0, [], []\n",
    "            with torch.no_grad():\n",
    "                for features, labels in self.test_loader:\n",
    "                    features, labels = features.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(features).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_preds.extend((outputs >= 0.5).float().cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(self.test_loader)\n",
    "            val_recall = recall_score(val_labels, val_preds)\n",
    "            \n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_recall\"].append(train_recall)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_recall\"].append(val_recall)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train Recall: {train_recall:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Recall: {val_recall:.4f}\"\n",
    "            )\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(self.model.state_dict(), self.output_dir / \"fraud_model.pth\")\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Evaluation metrics.\n",
    "        \"\"\"\n",
    "        logger.info(\"Evaluating model\")\n",
    "        self.model.eval()\n",
    "        preds, probs, labels = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, lbls in self.test_loader:\n",
    "                features = features.to(self.device)\n",
    "                outputs = self.model(features).squeeze()\n",
    "                preds.extend((outputs >= 0.5).float().cpu().numpy())\n",
    "                probs.extend(outputs.cpu().numpy())\n",
    "                labels.extend(lbls.numpy())\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision_score(labels, preds),\n",
    "            \"recall\": recall_score(labels, preds),\n",
    "            \"roc_auc\": roc_auc_score(labels, probs)\n",
    "        }\n",
    "        \n",
    "        # Compute ROC curve and optimal threshold\n",
    "        fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        metrics[\"optimal_threshold\"] = optimal_threshold\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {metrics['roc_auc']:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', label=f\"Threshold = {optimal_threshold:.2f}\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(self.output_dir / \"roc_curve.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(f\"Test Metrics: {metrics}\")\n",
    "        return metrics\n",
    "\n",
    "    def save_results(self, history: Dict[str, List[float]], metrics: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Save training history and evaluation metrics.\n",
    "\n",
    "        Args:\n",
    "            history (Dict[str, List[float]]): Training history.\n",
    "            metrics (Dict[str, float]): Evaluation metrics.\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving results\")\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.title(\"Loss Over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history[\"train_recall\"], label=\"Train Recall\")\n",
    "        plt.plot(history[\"val_recall\"], label=\"Validation Recall\")\n",
    "        plt.title(\"Recall Over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / \"training_history.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(self.output_dir / \"test_metrics.csv\", index=False)\n",
    "\n",
    "def main():\n",
    "    pipeline = FraudDetectionPipeline(\n",
    "        data_path=\"creditcard.csv\",  # Adjust to your path\n",
    "        output_dir=\"fraud_results\"\n",
    "    )\n",
    "    pipeline.load_data()\n",
    "    pipeline.visualize_data()\n",
    "    pipeline.preprocess_data()\n",
    "    pipeline.initialize_model(input_dim=30)\n",
    "    history = pipeline.train_model(epochs=20)\n",
    "    metrics = pipeline.evaluate√§lla\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FraudDetectionPipeline(\n",
    "    data_path=\"/path/to/creditcard.csv\",\n",
    "    output_dir=\"fraud_results\"\n",
    ")\n",
    "pipeline.load_data()\n",
    "pipeline.visualize_data()\n",
    "pipeline.preprocess_data()\n",
    "pipeline.initialize_model(input_dim=30)\n",
    "history = pipeline.train_model(epochs=20)\n",
    "metrics = pipeline.evaluate_model()\n",
    "pipeline.save_results(history, metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Sahidul-Credit-Card-Fraud-Detection.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
